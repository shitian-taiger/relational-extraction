{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[This, sentence, is, short, .]\n"
     ]
    }
   ],
   "source": [
    "# SpacyWordSplitter\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "# WordTokenizer: Wraps around SpacyWordSplitter\n",
    "from allennlp.data.tokenizers.word_tokenizer import WordTokenizer\n",
    "\n",
    "splitter = SpacyWordSplitter(pos_tags=True)\n",
    "tokenizer = WordTokenizer(word_splitter=splitter)\n",
    "\n",
    "sent_tokens = tokenizer.tokenize(\"This sentence is short.\") # Tokens\n",
    "sent_tokens = splitter.split_words(\"This sentence is short.\") # Tokens from Spacy (same)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_results(tagged_sentence):\n",
    "    \"\"\" Given raw tagged sentence, outputs all relations in whole sentences\n",
    "\n",
    "    Args:\n",
    "        tagged_sentence (json): Tagged from raw sentence\n",
    "    Returns:\n",
    "        rel_tuples: Tuples of form <arg1, rel, arg2>\n",
    "    \"\"\"\n",
    "    rel_tuples = []\n",
    "    for entry in tagged_sentence[\"verbs\"]:\n",
    "        phrases = re.findall(r\"\\[(.*?)\\]\", entry[\"description\"])\n",
    "        rel, pre_args, post_args = retrieve_tuples(phrases)\n",
    "        relation = \" \".join([word.split(\": \")[1] for word in rel])\n",
    "        tuples = [(pre.split(\": \")[1], relation, post.split(\": \")[1]) \\\n",
    "                        for pre in pre_args for post in post_args]\n",
    "        rel_tuples += tuples\n",
    "    return rel_tuples\n",
    "\n",
    "\n",
    "def retrieve_tuples(phrases):\n",
    "    \"\"\" Given BIO tagged phrases in a sentence, split into args and relationship\n",
    "\n",
    "    Args:\n",
    "        phrases (list): BIO tagged phrases\n",
    "    Returns:\n",
    "        rel, pre_args, post_args (list): separated phrases\n",
    "    \"\"\"\n",
    "    rel, pre_args, post_args = [], [], []\n",
    "    rel_found = False\n",
    "    for phrase in phrases:\n",
    "        if \"V:\" in phrase:\n",
    "            rel.append(phrase)\n",
    "            rel_found = True\n",
    "        elif not rel_found:\n",
    "            pre_args.append(phrase)\n",
    "        else:\n",
    "            post_args.append(phrase)\n",
    "    return rel, pre_args, post_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sentence': [This, sentence, is, short, .], 'predicate_index': 2}]\n"
     ]
    }
   ],
   "source": [
    "# Find all verbs in the input sentence\n",
    "pred_ids = [i for (i, t)\n",
    "            in enumerate(sent_tokens)\n",
    "            if t.pos_ == \"VERB\"]\n",
    "# Pair predicate index with sentence\n",
    "json_list = [{\"sentence\": sent_tokens,\n",
    "          \"predicate_index\": pred_id} for pred_id in pred_ids]\n",
    "print(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens\n",
      "TextField of length 5 with text: \n",
      " \t\t[This, sentence, is, short, .]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n",
      "verb_indicator\n",
      "SequenceLabelField of length 5 with labels:\n",
      " \t\t[0, 0, 1, 0, 0]\n",
      " \t\tin namespace: 'labels'.\n",
      "metadata\n",
      "MetadataField (print field.metadata to see specific information).\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.dataset_readers.semantic_role_labeling import SrlReader\n",
    "\n",
    "# DatasetReader is SemanticRoleLabeler\n",
    "dataset_reader = SrlReader()\n",
    "\n",
    "def json_to_instance(json_dict):\n",
    "    tokens = json_dict[\"sentence\"]\n",
    "    predicate_index = int(json_dict[\"predicate_index\"])\n",
    "    verb_labels = [0 for _ in tokens]\n",
    "    verb_labels[predicate_index] = 1\n",
    "    return dataset_reader.text_to_instance(tokens, verb_labels)\n",
    "\n",
    "instances = [json_to_instance(json_dict) for json_dict in json_list] # Representation of the sentence paired with a predicate\n",
    "\n",
    "# Fields within individual instances\n",
    "for field_name in instances[0].fields:\n",
    "    print(field_name)\n",
    "    field = instances[0][field_name]\n",
    "    print(field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taigemini/anaconda3/envs/oie/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n",
      "/Users/taigemini/anaconda3/envs/oie/lib/python3.6/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:51: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# This is temporary to facilitate exploration\n",
    "from allennlp.models.archival import Archive, load_archive\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "archived_oie = load_archive(\"https://s3-us-west-2.amazonaws.com/allennlp/models/openie-model.2018-08-20.tar.gz\")\n",
    "archived_srl = load_archive(\"https://s3-us-west-2.amazonaws.com/allennlp/models/srl-model-2018.05.25.tar.gz\")\n",
    "archived_ner = load_archive(\"https://s3-us-west-2.amazonaws.com/allennlp/models/ner-model-2018.12.18.tar.gz\")\n",
    "\n",
    "oie = Predictor.from_archive(archived_oie, \"open-information-extraction\")\n",
    "srl = Predictor.from_archive(archived_srl)\n",
    "ner = Predictor.from_archive(archived_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archived_custom = load_archive(\"./out_scratch/\")\n",
    "# custom = Predictor.from_archive(archived_custom, \"open-information-extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_model = srl._model\n",
    "oie_model = oie._model # Semantic_Role_Labeler \n",
    "encoder = oie_model.encoder # Seq2SeqWrapper\n",
    "module = encoder._module # StackedAlternatingLstm\n",
    "layers = module.lstm_layers # List of AugmentedLstms\n",
    "# print(len(layers))\n",
    "# print(layers[0])\n",
    "# print(module.input_size)\n",
    "# print(module.hidden_size)\n",
    "\n",
    "# Semantic Role Labeler output per Instance\n",
    "forward_out = oie_model.forward_on_instance(instances[0])\n",
    "keys = forward_out.keys() # Logits and class probabilities have shape (5, 62), Mask (5, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_results(predicted):\n",
    "    for verb in predicted[\"verbs\"]: # Print tags\n",
    "        print(verb[\"tags\"])\n",
    "    print(parse_results(predicted))\n",
    "\n",
    "# Predictor Instance\n",
    "sentences = \\\n",
    "    [\n",
    "        \"Courtalds' spinoff reflects pressure on British industry to boost share prices beyond the reach of corporate raiders.\",\n",
    "        \"The stock began trading at $14 apiece.\",\n",
    "        \"Mercury filling, particularly prevalent in the USA, was banned in the EU, partly because it causes antibiotic resistance\",\n",
    "        \"In 1971 , the FDA banned the use of Amphetamines after studies linked it to cancer and other problems in daughters of women who took the drug.\",\n",
    "        \"Instead there was a funeral, at st. francis de sales roman catholic church, in Belle_harbor, Queens, the parish of his birth.\",\n",
    "        \"FBI examined the relationship between Bin Laden and the Taliban\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 1971 , the FDA banned the use of Amphetamines after studies linked it to cancer and other problems in daughters of women who took the drug.\n",
      "================= OIE ===================\n",
      "['B-ARG3', 'I-ARG3', 'O', 'B-ARG0', 'I-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG0', 'O', 'B-V', 'B-ARG1', 'I-ARG1', 'O']\n",
      "[('In 1971', 'banned', 'the use of Amphetamines'), ('In 1971', 'banned', 'after studies linked it to cancer and other problems in daughters of women'), ('the FDA', 'banned', 'the use of Amphetamines'), ('the FDA', 'banned', 'after studies linked it to cancer and other problems in daughters of women'), ('studies', 'linked', 'it to cancer and other problems in daughters of women'), ('women', 'took', 'the drug')]\n",
      "================= NER ===================\n",
      "['O', 'O', 'O', 'O', 'U-ORG', 'O', 'O', 'O', 'O', 'U-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "1971 DATE\n",
      "FDA ORG\n",
      "Amphetamines GPE\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[3]\n",
    "print(sentence)\n",
    "\n",
    "print(\"================= OIE ===================\")\n",
    "predicted = oie.predict_json({\"sentence\": sentence})\n",
    "print_results(predicted)\n",
    "\n",
    "print(\"================= NER ===================\")\n",
    "predicted = ner.predict_json({\"sentence\": sentence})\n",
    "print(predicted[\"tags\"])\n",
    "# print_results(predicted)\n",
    "doc = nlp(sentence)\n",
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)\n",
    "\n",
    "# print(\"================= CUSTOM ================\")\n",
    "# predicted = custom.predict_json({\"sentence\": sentence})\n",
    "# print_results(predicted)\n",
    "\n",
    "# print(\"================= SRL ===================\")\n",
    "# predicted = srl.predict_json({\"sentence\": sentence})\n",
    "# for verb in predicted[\"verbs\"]:\n",
    "#     print(verb[\"tags\"])\n",
    "# print(parse_results(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
