{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[This, sentence, is, short, .]\n"
     ]
    }
   ],
   "source": [
    "# SpacyWordSplitter\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "# WordTokenizer: Wraps around SpacyWordSplitter\n",
    "from allennlp.data.tokenizers.word_tokenizer import WordTokenizer\n",
    "\n",
    "splitter = SpacyWordSplitter(pos_tags=True)\n",
    "tokenizer = WordTokenizer(word_splitter=splitter)\n",
    "\n",
    "sent_tokens = tokenizer.tokenize(\"This sentence is short.\") # Tokens\n",
    "sent_tokens = splitter.split_words(\"This sentence is short.\") # Tokens from Spacy (same)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_results(tagged_sentence):\n",
    "    \"\"\" Given raw tagged sentence, outputs all relations in whole sentences\n",
    "\n",
    "    Args:\n",
    "        tagged_sentence (json): Tagged from raw sentence\n",
    "    Returns:\n",
    "        rel_tuples: Tuples of form <arg1, rel, arg2>\n",
    "    \"\"\"\n",
    "    rel_tuples = []\n",
    "    for entry in tagged_sentence[\"verbs\"]:\n",
    "        phrases = re.findall(r\"\\[(.*?)\\]\", entry[\"description\"])\n",
    "        rel, pre_args, post_args = retrieve_tuples(phrases)\n",
    "        relation = \" \".join([word.split(\": \")[1] for word in rel])\n",
    "        tuples = [(pre.split(\": \")[1], relation, post.split(\": \")[1]) \\\n",
    "                        for pre in pre_args for post in post_args]\n",
    "        rel_tuples += tuples\n",
    "    return rel_tuples\n",
    "\n",
    "\n",
    "def retrieve_tuples(phrases):\n",
    "    \"\"\" Given BIO tagged phrases in a sentence, split into args and relationship\n",
    "\n",
    "    Args:\n",
    "        phrases (list): BIO tagged phrases\n",
    "    Returns:\n",
    "        rel, pre_args, post_args (list): separated phrases\n",
    "    \"\"\"\n",
    "    rel, pre_args, post_args = [], [], []\n",
    "    rel_found = False\n",
    "    for phrase in phrases:\n",
    "        if \"V:\" in phrase:\n",
    "            rel.append(phrase)\n",
    "            rel_found = True\n",
    "        elif not rel_found:\n",
    "            pre_args.append(phrase)\n",
    "        else:\n",
    "            post_args.append(phrase)\n",
    "    return rel, pre_args, post_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sentence': [This, sentence, is, short, .], 'predicate_index': 2}]\n"
     ]
    }
   ],
   "source": [
    "# Find all verbs in the input sentence\n",
    "pred_ids = [i for (i, t)\n",
    "            in enumerate(sent_tokens)\n",
    "            if t.pos_ == \"VERB\"]\n",
    "# Pair predicate index with sentence\n",
    "json_list = [{\"sentence\": sent_tokens,\n",
    "          \"predicate_index\": pred_id} for pred_id in pred_ids]\n",
    "print(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens\n",
      "TextField of length 5 with text: \n",
      " \t\t[This, sentence, is, short, .]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n",
      "verb_indicator\n",
      "SequenceLabelField of length 5 with labels:\n",
      " \t\t[0, 0, 1, 0, 0]\n",
      " \t\tin namespace: 'labels'.\n",
      "metadata\n",
      "MetadataField (print field.metadata to see specific information).\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.dataset_readers.semantic_role_labeling import SrlReader\n",
    "\n",
    "# DatasetReader is SemanticRoleLabeler\n",
    "dataset_reader = SrlReader()\n",
    "\n",
    "def json_to_instance(json_dict):\n",
    "    tokens = json_dict[\"sentence\"]\n",
    "    predicate_index = int(json_dict[\"predicate_index\"])\n",
    "    verb_labels = [0 for _ in tokens]\n",
    "    verb_labels[predicate_index] = 1\n",
    "    return dataset_reader.text_to_instance(tokens, verb_labels)\n",
    "\n",
    "instances = [json_to_instance(json_dict) for json_dict in json_list] # Representation of the sentence paired with a predicate\n",
    "\n",
    "# Fields within individual instances\n",
    "for field_name in instances[0].fields:\n",
    "    print(field_name)\n",
    "    field = instances[0][field_name]\n",
    "    print(field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:allennlp.nn.initializers:Did not use initialization regex that was passed: .*weight_hh.*\n",
      "WARNING:allennlp.nn.initializers:Did not use initialization regex that was passed: .*bias_hh.*\n",
      "WARNING:allennlp.nn.initializers:Did not use initialization regex that was passed: .*bias_ih.*\n",
      "WARNING:allennlp.nn.initializers:Did not use initialization regex that was passed: .*weight_ih.*\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# This is temporary to facilitate exploration\n",
    "from allennlp.models.archival import Archive, load_archive\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "archived_oie = load_archive(\"https://s3-us-west-2.amazonaws.com/allennlp/models/openie-model.2018-08-20.tar.gz\")\n",
    "archived_srl = load_archive(\"https://s3-us-west-2.amazonaws.com/allennlp/models/srl-model-2018.05.25.tar.gz\")\n",
    "archived_ner = load_archive(\"https://s3-us-west-2.amazonaws.com/allennlp/models/ner-model-2018.12.18.tar.gz\")\n",
    "archived_dp = load_archive(\"https://s3-us-west-2.amazonaws.com/allennlp/models/biaffine-dependency-parser-ptb-2018.08.23.tar.gz\")\n",
    "\n",
    "oie = Predictor.from_archive(archived_oie, \"open-information-extraction\")\n",
    "srl = Predictor.from_archive(archived_srl)\n",
    "ner = Predictor.from_archive(archived_ner)\n",
    "dp = Predictor.from_archive(archived_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archived_custom = load_archive(\"./out_scratch/\")\n",
    "# custom = Predictor.from_archive(archived_custom, \"open-information-extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_model = srl._model\n",
    "oie_model = oie._model # Semantic_Role_Labeler \n",
    "encoder = oie_model.encoder # Seq2SeqWrapper\n",
    "module = encoder._module # StackedAlternatingLstm\n",
    "layers = module.lstm_layers # List of AugmentedLstms\n",
    "# print(len(layers))\n",
    "# print(layers[0])\n",
    "# print(module.input_size)\n",
    "# print(module.hidden_size)\n",
    "\n",
    "# Semantic Role Labeler output per Instance\n",
    "forward_out = oie_model.forward_on_instance(instances[0])\n",
    "keys = forward_out.keys() # Logits and class probabilities have shape (5, 62), Mask (5, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_results(predicted):\n",
    "    for verb in predicted[\"verbs\"]: # Print tags\n",
    "        print(verb[\"tags\"])\n",
    "    print(parse_results(predicted))\n",
    "\n",
    "# Predictor Instance\n",
    "sentences = \\\n",
    "    [\n",
    "        \"Courtalds' spinoff reflects pressure on British industry to boost share prices beyond the reach of corporate raiders.\",\n",
    "        \"The stock began trading at $14 apiece.\",\n",
    "        \"Mercury filling, particularly prevalent in the USA, was banned in the EU, partly because it causes antibiotic resistance\",\n",
    "        \"In 1971 , the FDA banned the use of Amphetamines after studies linked it to cancer and other problems in daughters of women who took the drug.\",\n",
    "        \"Instead there was a funeral, at st. francis de sales roman catholic church, in Belle_harbor, Queens, the parish of his birth.\",\n",
    "        \"FBI examined the relationship between Bin Laden and the Taliban\",\n",
    "        \"Deva has moved to Mumbai and is residing at Boney Kapoor's old place called Green Acres.\",\n",
    "        \"Did Uriah honestly think he could beat The Legend of Zelda in under three hours.\",\n",
    "        \"Mr. Agnew was vice president of the U.S. from 1969 until he resigned in 1973.\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Agnew was vice president of the U.S. from 1969 until he resigned in 1973.\n",
      "================= OIE ===================\n",
      "['B-ARG0', 'I-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-ARG2', 'I-ARG2', 'B-ARG3', 'I-ARG3', 'I-ARG3', 'I-ARG3', 'I-ARG3', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'O']\n",
      "[('Mr. Agnew', 'was', 'vice president of the U.S.'), ('Mr. Agnew', 'was', 'from 1969'), ('Mr. Agnew', 'was', 'until he resigned in 1973'), ('he', 'resigned', 'in 1973')]\n",
      "================= NER ===================\n",
      "['O', 'U-PER', 'O', 'O', 'O', 'O', 'O', 'U-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "+++++ SPACY +++++\n",
      "Agnew PERSON\n",
      "U.S. GPE\n",
      "1969 DATE\n",
      "1973 DATE\n",
      "================= DP ===================\n",
      "president\n",
      "['NN']\n",
      "{'word': 'Agnew', 'nodeType': 'nsubj', 'attributes': ['NNP'], 'link': 'nsubj', 'spans': [{'start': 4, 'end': 10}], 'children': [{'word': 'Mr.', 'nodeType': 'nn', 'attributes': ['NNP'], 'link': 'nn', 'spans': [{'start': 0, 'end': 4}]}]}\n",
      "{'word': 'was', 'nodeType': 'cop', 'attributes': ['VBD'], 'link': 'cop', 'spans': [{'start': 10, 'end': 14}]}\n",
      "{'word': 'vice', 'nodeType': 'nn', 'attributes': ['NN'], 'link': 'nn', 'spans': [{'start': 14, 'end': 19}]}\n",
      "{'word': 'of', 'nodeType': 'prep', 'attributes': ['IN'], 'link': 'prep', 'spans': [{'start': 29, 'end': 32}], 'children': [{'word': 'U.S.', 'nodeType': 'pobj', 'attributes': ['NNP'], 'link': 'pobj', 'spans': [{'start': 36, 'end': 41}], 'children': [{'word': 'the', 'nodeType': 'det', 'attributes': ['DT'], 'link': 'det', 'spans': [{'start': 32, 'end': 36}]}]}]}\n",
      "{'word': 'from', 'nodeType': 'prep', 'attributes': ['IN'], 'link': 'prep', 'spans': [{'start': 41, 'end': 46}], 'children': [{'word': '1969', 'nodeType': 'pobj', 'attributes': ['CD'], 'link': 'pobj', 'spans': [{'start': 46, 'end': 51}]}]}\n",
      "{'word': 'resigned', 'nodeType': 'advcl', 'attributes': ['VBD'], 'link': 'advcl', 'spans': [{'start': 60, 'end': 69}], 'children': [{'word': 'until', 'nodeType': 'mark', 'attributes': ['IN'], 'link': 'mark', 'spans': [{'start': 51, 'end': 57}]}, {'word': 'he', 'nodeType': 'nsubj', 'attributes': ['PRP'], 'link': 'nsubj', 'spans': [{'start': 57, 'end': 60}]}, {'word': 'in', 'nodeType': 'prep', 'attributes': ['IN'], 'link': 'prep', 'spans': [{'start': 69, 'end': 72}], 'children': [{'word': '1973', 'nodeType': 'pobj', 'attributes': ['CD'], 'link': 'pobj', 'spans': [{'start': 72, 'end': 77}]}]}]}\n",
      "{'word': '.', 'nodeType': 'punct', 'attributes': ['.'], 'link': 'punct', 'spans': [{'start': 77, 'end': 79}]}\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[8]\n",
    "print(sentence)\n",
    "\n",
    "print(\"================= OIE ===================\")\n",
    "predicted = oie.predict_json({\"sentence\": sentence})\n",
    "print_results(predicted)\n",
    "\n",
    "print(\"================= NER ===================\")\n",
    "predicted = ner.predict_json({\"sentence\": sentence})\n",
    "print(predicted[\"tags\"])\n",
    "# print_results(predicted)\n",
    "doc = nlp(sentence)\n",
    "print(\"+++++ SPACY +++++\")\n",
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)\n",
    "    \n",
    "print(\"================= DP ===================\")\n",
    "predicted = dp.predict_json({\"sentence\": sentence})\n",
    "tree = predicted['hierplane_tree']\n",
    "root = tree['root']\n",
    "print(root['word'])\n",
    "print(root['attributes'])\n",
    "for child in root['children']:\n",
    "    print(child)\n",
    "\n",
    "\n",
    "\n",
    "# print(\"================= CUSTOM ================\")\n",
    "# predicted = custom.predict_json({\"sentence\": sentence})\n",
    "# print_results(predicted)\n",
    "\n",
    "# print(\"================= SRL ===================\")\n",
    "# predicted = srl.predict_json({\"sentence\": sentence})\n",
    "# for verb in predicted[\"verbs\"]:\n",
    "#     print(verb[\"tags\"])\n",
    "# print(parse_results(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
